# Async Workflow Orchestrator

[![Go Version](https://img.shields.io/badge/Go-1.25.1-blue.svg)](https://golang.org)
[![License](https://img.shields.io/badge/License-Apache%202.0-green.svg)](LICENSE)

A high-performance, event-driven workflow orchestration system built with Go, designed to handle complex asynchronous task processing at scale. This system leverages Apache Kafka for reliable event streaming, PostgreSQL for persistent storage, and Redis for caching, providing a robust foundation for building distributed workflow applications.

## üéØ Overview

Async Workflow Orchestrator is a production-ready backend system that enables you to define, trigger, and manage complex workflows through a declarative DSL. It implements a state machine pattern to orchestrate multi-step processes, providing features like automatic retries, failure handling, task chaining, and comprehensive observability.

**Key Features:**
- üîÑ **Event-Driven Architecture**: Built on Apache Kafka for reliable, scalable message processing
- üé≠ **State Machine Orchestration**: Intelligent workflow coordination with automatic step transitions
- üîÅ **Automatic Retry Logic**: Built-in retry mechanisms with configurable policies
- üìä **Workflow Instance Tracking**: Full visibility into workflow execution state and history
- üîå **Extensible Task System**: Easy integration of custom task executors
- üìù **Declarative Workflow DSL**: Define workflows using YAML configuration
- üèóÔ∏è **Microservices Architecture**: Separate API, Orchestrator, and Worker services
- üìà **Production-Ready**: Structured logging, health checks, and graceful shutdown

## üèóÔ∏è Architecture

The system follows a microservices architecture with three core components:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ             ‚îÇ  HTTP   ‚îÇ              ‚îÇ  Kafka  ‚îÇ             ‚îÇ
‚îÇ  API Server ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ Orchestrator ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ   Workers   ‚îÇ
‚îÇ             ‚îÇ         ‚îÇ              ‚îÇ         ‚îÇ             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ                       ‚îÇ                        ‚îÇ
       ‚îÇ                       ‚îÇ                        ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ                     ‚îÇ
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ PostgreSQL ‚îÇ        ‚îÇ    Redis    ‚îÇ
            ‚îÇ            ‚îÇ        ‚îÇ             ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Components

1. **API Service** (`cmd/api/`): RESTful API endpoints for workflow management
2. **Orchestrator Service** (`cmd/orchestrator/`): Core workflow coordination and state machine management
3. **Worker Service** (`cmd/worker/`): Task execution and processing

### Data Flow

1. Client submits workflow request via REST API
2. API creates workflow instance and publishes initial task event to Kafka
3. Workers consume task events, execute tasks, and publish completion events
4. Orchestrator consumes completions, updates state, and triggers next tasks
5. Process continues until workflow completes or fails

## üöÄ Getting Started

### Prerequisites

- **Go**: 1.25.1 or higher
- **Docker**: For running dependencies
- **Docker Compose**: For local development environment
- **Task**: Task runner (optional, for convenience commands)

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/Vighnesh-V-H/async.git
   cd async
   ```

2. **Set up environment variables**
   ```bash
   cp .env.example .env
   # Edit .env with your configuration
   ```

   Required environment variables:
   ```env
   # Database
   DATABASE_URL=postgres://pg:pradyuman@localhost:5432/db?sslmode=disable

   # Kafka
   KAFKA_BROKERS=localhost:29092

   # Redis
   REDIS_URL=localhost:6379

   # Application
   PORT=8080
   LOG_LEVEL=info
   LOG_FORMAT=json
   ENVIRONMENT=development
   ```

3. **Start infrastructure services**
   ```bash
   docker-compose up -d
   ```

   This will start:
   - PostgreSQL (port 5432)
   - Adminer (port 7080) - Database management UI
   - Redis (port 6379)
   - Kafka (ports 9092, 29092)
   - Kafka UI (port 8081)

4. **Run database migrations**
   ```bash
   task migrations:up
   ```

5. **Start the orchestrator service**
   ```bash
   task run:orc
   # Or manually: go run cmd/orchestrator/main.go
   ```

### Quick Start Example

1. **Create a workflow**
   ```bash
   curl -X POST http://localhost:8080/workflow/create \
     -H "Content-Type: application/json" \
     -d '{
       "name": "audio-generation",
       "event": "generate_audio",
       "message": "Audio generation workflow",
       "steps": 3,
       "handler_url": "http://worker:8082/tasks"
     }'
   ```

2. **Trigger audio generation**
   ```bash
   curl -X POST http://localhost:8080/audio/generate \
     -H "Content-Type: application/json" \
     -d '{
       "text": "Hello, world!",
       "voice": "en-US-Standard-A",
       "metadata": {
         "format": "mp3",
         "sample_rate": 44100
       }
     }'
   ```

3. **Check execution status**
   ```bash
   curl http://localhost:8080/audio/status/{execution_id}
   ```

## üìÅ Project Structure

```
async/
‚îú‚îÄ‚îÄ cmd/                        # Application entry points
‚îÇ   ‚îú‚îÄ‚îÄ api/                    # REST API server
‚îÇ   ‚îú‚îÄ‚îÄ orchestrator/           # Workflow orchestrator service
‚îÇ   ‚îî‚îÄ‚îÄ worker/                 # Task worker service
‚îú‚îÄ‚îÄ internal/                   # Private application code
‚îÇ   ‚îú‚îÄ‚îÄ events/                 # Kafka producer/consumer
‚îÇ   ‚îú‚îÄ‚îÄ handler/                # HTTP request handlers
‚îÇ   ‚îú‚îÄ‚îÄ logger/                 # Structured logging
‚îÇ   ‚îú‚îÄ‚îÄ models/                 # Domain models and entities
‚îÇ   ‚îú‚îÄ‚îÄ orchestrator/           # State machine logic
‚îÇ   ‚îú‚îÄ‚îÄ repositories/           # Data access layer
‚îÇ   ‚îú‚îÄ‚îÄ router/                 # HTTP route definitions
‚îÇ   ‚îú‚îÄ‚îÄ service/                # Business logic layer
‚îÇ   ‚îî‚îÄ‚îÄ workers/                # Task executors
‚îú‚îÄ‚îÄ pkg/                        # Public reusable packages
‚îÇ   ‚îú‚îÄ‚îÄ cache/                  # Redis caching
‚îÇ   ‚îú‚îÄ‚îÄ database/               # PostgreSQL connection and migrations
‚îÇ   ‚îú‚îÄ‚îÄ dsl/                    # Workflow DSL parser (planned)
‚îÇ   ‚îú‚îÄ‚îÄ kafka/                  # Kafka client initialization
‚îÇ   ‚îî‚îÄ‚îÄ observability/          # Metrics and tracing (planned)
‚îú‚îÄ‚îÄ configs/                    # Configuration files
‚îú‚îÄ‚îÄ deploy/                     # Deployment manifests
‚îú‚îÄ‚îÄ workflows/                  # Workflow definitions (YAML)
‚îî‚îÄ‚îÄ docker-compose.yml          # Local development stack
```

## üîß Development

### Available Tasks

Using [Task](https://taskfile.dev/), you can run:

```bash
task help              # List all available tasks
task run:orc           # Run orchestrator service
task migrations:new    # Create new migration
task migrations:up     # Apply migrations
task migrations:down   # Rollback last migration
task tidy              # Format code and tidy dependencies
```

### Code Organization

The project follows clean architecture principles:

- **Models** (`internal/models`): Core domain entities with GORM annotations
- **Repositories** (`internal/repositories`): Data persistence layer
- **Services** (`internal/service`): Business logic and orchestration
- **Handlers** (`internal/handler`): HTTP request/response handling
- **Events** (`internal/events`): Kafka event publishing and consumption

## üìä Database Schema

The system uses PostgreSQL with the following core tables:

- `workflows`: Workflow definitions and metadata
- `workflow_instances`: Individual workflow execution instances
- `tasks`: Task execution records
- `history_entries`: Audit trail of workflow events
- `workflow_registries`: Worker registration and health tracking

## üé≠ Workflow DSL

Workflows are defined using a YAML-based DSL. Example workflow structure:

```yaml
name: audio-generation-workflow
version: 1.0
triggers:
  - type: webhook
    event: "generate_audio"

states:
  - id: extract_text
    type: ai_task
    model: "gpt-4"
    on_success: convert_to_audio
    retries: 2
    timeout: 60s

  - id: convert_to_audio
    type: task
    action: "tts_generate"
    on_success: upload_audio
    
  - id: upload_audio
    type: http_call
    method: POST
    url: "https://storage.api/store/audio"
    on_success: send_notification
    retries: 3
```

## üîç Monitoring & Observability

### Health Checks

```bash
curl http://localhost:8080/health
```

### Logs

All services use structured JSON logging with zerolog:
- Configurable log levels (debug, info, warn, error)
- Request tracing with execution IDs
- Contextual metadata for debugging

### Kafka UI

Access Kafka UI at http://localhost:8081 to monitor:
- Topic messages
- Consumer groups
- Broker health

### Database Admin

Access Adminer at http://localhost:7080 to:
- View database tables
- Run SQL queries
- Monitor workflow instances

## üìã TODO & Roadmap

### High Priority - Core Features
- [ ] **Authentication & Authorization**: Implement JWT-based auth middleware for API endpoints
- [ ] **DSL Parser Implementation**: Complete YAML workflow DSL parser in `pkg/dsl/`
- [ ] **Worker Service**: Implement actual worker service in `cmd/worker/main.go`
- [ ] **Task Executors**: Build pluggable task executor system with sample implementations
- [ ] **Retry & Timeout Logic**: Implement exponential backoff and task timeout handling
- [ ] **Dead Letter Queue**: Add DLQ for failed tasks with manual intervention support

### Medium Priority - Production Readiness
- [ ] **API Rate Limiting**: Add rate limiting middleware to prevent abuse
- [ ] **Request Validation**: Comprehensive input validation for all API endpoints
- [ ] **Error Handling**: Standardized error responses and error codes
- [ ] **Graceful Degradation**: Circuit breakers for external service calls
- [ ] **Database Connection Pooling**: Optimize database connections with proper pooling
- [ ] **Metrics & Tracing**: Integrate Prometheus metrics and distributed tracing
- [ ] **API Documentation**: Generate OpenAPI/Swagger documentation
- [ ] **Unit & Integration Tests**: Comprehensive test coverage (target: >80%)

### Low Priority - Enhanced Features
- [ ] **Webhook Support**: Implement webhook notifications for workflow events
- [ ] **Conditional Branching**: Support for complex workflow conditions
- [ ] **Parallel Task Execution**: Execute multiple tasks concurrently
- [ ] **Scheduled Workflows**: Cron-based workflow triggers
- [ ] **Workflow Versioning**: Support multiple versions of same workflow
- [ ] **Admin Dashboard**: Web UI for workflow management
- [ ] **CLI Tool**: Command-line tool for workflow operations
- [ ] **Multi-tenancy**: Support for isolated tenant workspaces

### Infrastructure & DevOps
- [ ] **Kubernetes Deployment**: Helm charts for K8s deployment
- [ ] **CI/CD Pipeline**: GitHub Actions for automated testing and deployment
- [ ] **Docker Images**: Multi-stage Docker builds for all services
- [ ] **Performance Testing**: Load testing and benchmarking
- [ ] **Security Audit**: Dependency scanning and security best practices
- [ ] **Backup & Recovery**: Automated database backup strategy

## ü§ù Contributing

Contributions are welcome! Please follow these guidelines:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## üìÑ License

This project is licensed under the Apache License 2.0 - see the LICENSE file for details.

## üôè Acknowledgments

Built with these excellent open-source projects:
- [Gin](https://github.com/gin-gonic/gin) - HTTP web framework
- [GORM](https://gorm.io/) - ORM library
- [Confluent Kafka Go](https://github.com/confluentinc/confluent-kafka-go) - Kafka client
- [Zerolog](https://github.com/rs/zerolog) - Structured logging
- [Redis](https://redis.io/) - In-memory data store
- [PostgreSQL](https://www.postgresql.org/) - Relational database

## üìß Contact

For questions and support, please open an issue on GitHub.

---

**Note**: This is an active development project. APIs and features may change. Check the TODO section for planned improvements and current limitations.